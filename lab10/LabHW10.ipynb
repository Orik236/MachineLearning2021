{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learnining: Lab and HW 10\n",
    "### Homework Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid, linear\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "def linear_prime(x):\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18573247 -0.32741703  0.75667271]\n",
      " [-0.77501904 -0.65274808 -0.76636578]\n",
      " [ 0.70895608  0.04280052  0.031176  ]]\n",
      "[[0.03300837]\n",
      " [0.12702327]\n",
      " [0.74660695]]\n",
      "[0 0] [0.41618001]\n",
      "[0 1] [0.44545231]\n",
      "[1 0] [-0.11979862]\n",
      "[1 1] [-0.07313723]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [4.09959286e-06]\n",
      "[0 1] [0.99587589]\n",
      "[1 0] [0.99747347]\n",
      "[1 1] [-0.00025312]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "#         self.activation = linear\n",
    "#         self.activation_prime = linear_prime\n",
    "#         self.activation = sigmoid\n",
    "#         self.activation_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23868577  0.8312308   0.25486834]\n",
      " [-0.62234113 -0.75747164  0.91175658]\n",
      " [-0.60107436 -0.11527915  0.97948007]]\n",
      "[[-0.81099854]\n",
      " [-0.30490138]\n",
      " [ 0.34569002]]\n",
      "[0 0] [0.41122302]\n",
      "[0 1] [0.6230006]\n",
      "[1 0] [0.66639952]\n",
      "[1 1] [0.8253698]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [0.49996246]\n",
      "[0 1] [0.49952033]\n",
      "[1 0] [0.50072063]\n",
      "[1 1] [0.5002785]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class XOR:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation_linear = linear\n",
    "        self.activation_linear_prime = linear_prime\n",
    "        self.activation_sigmoid = sigmoid\n",
    "        self.activation_sigmoid_prime = sigmoid_prime\n",
    "        \n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        self.moments = []\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "            self.moments.append(np.zeros(r.shape))\n",
    "            print(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        print(r)\n",
    "        self.weights.append(r)\n",
    "        self.moments.append(np.zeros(r.shape))\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000, gamma=0.1):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        errors = []\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)-1):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation_linear(dot_value)\n",
    "                    a.append(activation)\n",
    "            a.append(self.activation_sigmoid(np.dot(a[-1], self.weights[-1])))\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            errors.append(error**2)\n",
    "            deltas = [error * self.activation_sigmoid_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_linear_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                prev_weight = np.copy(self.weights[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta) + gamma*self.moments[i]\n",
    "                self.moments[i] = self.weights[i]-prev_weight\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                \n",
    "        return errors\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(len(self.weights)-1):\n",
    "            a = self.activation_linear(np.dot(a, self.weights[l]))\n",
    "        return self.activation_sigmoid(np.dot(a, self.weights[-1]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = XOR([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "    epochs = 100000\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "    errors = nn.fit(X, y, epochs=epochs, learning_rate=0.2, gamma=0.1)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))\n",
    "        \n",
    "    plt.plot(np.arange(epochs), errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
